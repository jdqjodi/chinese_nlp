{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# import dependencies\n",
    "import tika\n",
    "tika.initVM()\n",
    "from tika import parser\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "# web resources used\n",
    "# https://www.kaggle.com/itratrahman/nlp-tutorial-using-python\n",
    "    # ideally parse pdf data into dataframe format for feature engineering?\n",
    "# https://stackoverflow.com/questions/50985619/how-to-read-pdf-files-which-are-in-asian-languages-chinese-japanese-thai-etc - not that useful but oh well\n",
    "# https://stackoverflow.com/questions/46389254/how-to-parse-text-extracted-from-pdf-file-with-delimiter-using-python - answer by Grijesh Chauhan\n",
    "# various other general programming google search results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# regex for dealing with strings of text\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in pdf\n",
    "parsed = parser.from_file('AAAI-19_Accepted_Papers.pdf')\n",
    "metadata = parsed[\"metadata\"]\n",
    "content = parsed[\"content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checked content, is a string type, looks good when printed but is a load of formatting when called\n",
    "# re split at new lines and \\xa0\n",
    "split_at_nums = re.split(r'\\s+(?=\\d)|(?<=\\d)\\s+', content)\n",
    "# split_at_nums = re.split(\"\\d+:?\", content)\n",
    "# this isn't that great because it's splitting at any instance that says 3D\n",
    "# extractedText = re.split('\\n|\\xa0', content) # parsedText = [''.join([*filter(str.isalnum, e)]) for e in extractedText]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1179"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entries = [re.sub('\\xa0|\\n|\\xad', '', line) for line in split_at_nums]\n",
    "len(entries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Qiao\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# https://blog.manash.me/configuring-stanford-parser-and-stanford-ner-tagger-with-nltk-in-python-on-windows-f685483c374a\n",
    "\n",
    "import nltk\n",
    "import os\n",
    "\n",
    "# core nlp needs java 8 to run\n",
    "import os\n",
    "java_path = \"C:\\Program Files (x86)\\Common Files\\Oracle\\Java\\javapath\\java.exe\"\n",
    "os.environ['JAVAHOME'] = java_path\n",
    "\n",
    "# downloaded punkt separately because error text demanded it\n",
    "nltk.download('punkt')\n",
    "from nltk.tag import StanfordNERTagger\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# change the path according to my system\n",
    "stanford_classifier = r\"C:\\Users\\Qiao\\Downloads\\sdnlp\\stanford-ner-2018-10-16\\stanford-ner-2018-10-16\\classifiers\\english.all.3class.distsim.crf.ser.gz\"\n",
    "stanford_ner_path = r\"C:\\Users\\Qiao\\Downloads\\sdnlp\\stanford-ner-2018-10-16\\stanford-ner-2018-10-16\\stanford-ner.jar\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('81', 'O'), (':', 'O'), ('PhoneMD', 'O'), (':', 'O'), ('Learning', 'O'), ('to', 'O'), ('Diagnose', 'O'), ('Parkinson', 'O'), (\"'s\", 'O'), ('Disease', 'O'), ('from', 'O'), ('Smartphone', 'ORGANIZATION'), ('Data', 'ORGANIZATION'), ('Patrick', 'ORGANIZATION'), ('Schwab', 'ORGANIZATION'), ('(', 'ORGANIZATION'), ('ETH', 'ORGANIZATION'), ('Zurich', 'ORGANIZATION'), (')', 'O'), ('*', 'O'), (';', 'O'), ('Walter', 'PERSON'), ('Karlen', 'PERSON'), ('(', 'O'), ('ETH', 'O'), ('Zurich', 'LOCATION'), (')', 'O')]\n"
     ]
    }
   ],
   "source": [
    "# Creating Tagger Object\n",
    "# checking to see if it worked\n",
    "# takes a long time to process entries line by line, will process single file then clean up\n",
    "st = StanfordNERTagger(stanford_classifier, stanford_ner_path, encoding='utf-8')\n",
    "\n",
    "test_text = entries[11]\n",
    "\n",
    "test_tokenized_text = word_tokenize(test_text)\n",
    "test_classified_text = st.tag(test_tokenized_text)\n",
    "\n",
    "print(test_classified_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean up formatters in single file from parsed content\n",
    "single_file = re.sub('\\xa0|\\n|\\xad', '', content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply to string parsed from pdf\n",
    "text = single_file\n",
    "\n",
    "tokenized_text = word_tokenize(text)\n",
    "classified_text = st.tag(tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to dataframe format\n",
    "df = pd.DataFrame(classified_text[92:]\n",
    "                 , columns=['tokenized_text','tag']) # content after header\n",
    "col_list = df.columns\n",
    "#df.loc[(df[col_list] == 'O').any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# looping to find each tag in tuples too long, I can't optimize this; using dataframes to make categories\n",
    "raw_context = pd.DataFrame(df.loc[df['tag'] == 'O', 'tokenized_text'], index=None)\n",
    "raw_persons = pd.DataFrame(df.loc[df['tag'] == 'PERSON', 'tokenized_text'], index=None)\n",
    "raw_org = pd.DataFrame(df.loc[df['tag'] == 'ORGANIZATION', 'tokenized_text'], index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = raw_context['tokenized_text'].values.tolist()\n",
    "persons = raw_persons['tokenized_text'].values.tolist()\n",
    "orgs = raw_org['tokenized_text'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# manual data cleaning phase\n",
    "# organization tags\n",
    "temp_list = []\n",
    "orgs_list = []\n",
    "for item in orgs:\n",
    "    if item not in \"()*\": # assuming vast majority of organizatins delimited by round brackets\n",
    "        temp_list.append(item)\n",
    "    else:\n",
    "        orgs_list.append(' '.join(temp_list))\n",
    "        temp_list = []\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# O tags, most titles are categorized under such\n",
    "temp_list = []\n",
    "cleaned_list = []\n",
    "context_list = []\n",
    "for item in context:\n",
    "    if item not in \"()*^; \":\n",
    "        temp_list.append(item)\n",
    "    else:\n",
    "        cleaned_list.append(' '.join(temp_list))\n",
    "        temp_list = []\n",
    "        pass\n",
    "cleaned_list = filter(None, cleaned_list) # dropped empty str\n",
    "for item in cleaned_list:\n",
    "    if item[0].isdigit():\n",
    "        context_list.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# person tags\n",
    "names_list = []\n",
    "for item in persons:\n",
    "    if len(item) > 1 and \".\" not in item and item not in \"()*^; \": # I'm going to get rid of middle initials..\n",
    "        names_list.append(item)\n",
    "    else:\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final dataframes for extracted person names, organizations, and paper titles\n",
    "# or something of those sorts\n",
    "names_df = pd.DataFrame(names_list,\n",
    "                       columns=['names'])\n",
    "organizations_df = pd.DataFrame(orgs_list,\n",
    "                       columns=['organizations'])\n",
    "titles_df = pd.DataFrame(context_list,\n",
    "                       columns=['titles'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "names_df.to_csv('names_df')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "organizations_df.to_csv('organizations_df')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles_df.to_csv('titles_df')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
