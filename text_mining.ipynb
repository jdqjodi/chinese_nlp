{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# import dependencies\n",
    "import tika\n",
    "tika.initVM()\n",
    "from tika import parser\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "# web resources used\n",
    "# https://www.kaggle.com/itratrahman/nlp-tutorial-using-python\n",
    "    # ideally parse pdf data into dataframe format for feature engineering?\n",
    "# https://stackoverflow.com/questions/50985619/how-to-read-pdf-files-which-are-in-asian-languages-chinese-japanese-thai-etc - not that useful but oh well\n",
    "# https://stackoverflow.com/questions/46389254/how-to-parse-text-extracted-from-pdf-file-with-delimiter-using-python - answer by Grijesh Chauhan\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed = parser.from_file('AAAI-19_Accepted_Papers.pdf')\n",
    "metadata = parsed[\"metadata\"]\n",
    "content = parsed[\"content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checked content, is a string type, looks good when printed but is a load of formatting when called\n",
    "# re split at new lines and \\xa0\n",
    "split_at_nums = re.split(r'\\s+(?=\\d)|(?<=\\d)\\s+', content)\n",
    "# split_at_nums = re.split(\"\\d+:?\", content)\n",
    "# this isn't that great because it's splitting at any instance that says 3D\n",
    "# extractedText = re.split('\\n|\\xa0', content) # parsedText = [''.join([*filter(str.isalnum, e)]) for e in extractedText]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1179"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entries = [re.sub('\\xa0|\\n|\\xad', '', line) for line in split_at_nums]\n",
    "len(entries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Qiao\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# https://blog.manash.me/configuring-stanford-parser-and-stanford-ner-tagger-with-nltk-in-python-on-windows-f685483c374a\n",
    "\n",
    "import nltk\n",
    "import os\n",
    "\n",
    "# core nlp needs java 8 to run\n",
    "import os\n",
    "java_path = \"C:\\Program Files (x86)\\Common Files\\Oracle\\Java\\javapath\\java.exe\"\n",
    "os.environ['JAVAHOME'] = java_path\n",
    "\n",
    "# downloaded punkt separately because error text demanded it\n",
    "nltk.download('punkt')\n",
    "from nltk.tag import StanfordNERTagger\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# change the path according to my system\n",
    "stanford_classifier = r\"C:\\Users\\Qiao\\Downloads\\sdnlp\\stanford-ner-2018-10-16\\stanford-ner-2018-10-16\\classifiers\\english.all.3class.distsim.crf.ser.gz\"\n",
    "stanford_ner_path = r\"C:\\Users\\Qiao\\Downloads\\sdnlp\\stanford-ner-2018-10-16\\stanford-ner-2018-10-16\\stanford-ner.jar\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('81', 'O'), (':', 'O'), ('PhoneMD', 'O'), (':', 'O'), ('Learning', 'O'), ('to', 'O'), ('Diagnose', 'O'), ('Parkinson', 'O'), (\"'s\", 'O'), ('Disease', 'O'), ('from', 'O'), ('Smartphone', 'ORGANIZATION'), ('Data', 'ORGANIZATION'), ('Patrick', 'ORGANIZATION'), ('Schwab', 'ORGANIZATION'), ('(', 'ORGANIZATION'), ('ETH', 'ORGANIZATION'), ('Zurich', 'ORGANIZATION'), (')', 'O'), ('*', 'O'), (';', 'O'), ('Walter', 'PERSON'), ('Karlen', 'PERSON'), ('(', 'O'), ('ETH', 'O'), ('Zurich', 'LOCATION'), (')', 'O')]\n"
     ]
    }
   ],
   "source": [
    "# Creating Tagger Object\n",
    "# checking to see if it worked\n",
    "# takes a long time to process entries line by line, will process single file then clean up\n",
    "st = StanfordNERTagger(stanford_classifier, stanford_ner_path, encoding='utf-8')\n",
    "\n",
    "text = entries[11]\n",
    "\n",
    "tokenized_text = word_tokenize(text)\n",
    "classified_text = st.tag(tokenized_text)\n",
    "\n",
    "print(classified_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean up formatters in single file from parsed content\n",
    "single_file = re.sub('\\xa0|\\n|\\xad', '', content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = single_file\n",
    "\n",
    "tokenized_text = word_tokenize(text)\n",
    "classified_text = st.tag(tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(classified_text[92:]\n",
    "                 , columns=['tokenized_text','tag']) # content after header\n",
    "col_list = df.columns\n",
    "#df.loc[(df[col_list] == 'O').any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# looping to find each tag in tuples too long, I can't optimize this; using dataframes to make categories\n",
    "raw_context = pd.DataFrame(df.loc[df['tag'] == 'O', 'tokenized_text'], index=None)\n",
    "raw_persons = pd.DataFrame(df.loc[df['tag'] == 'PERSON', 'tokenized_text'], index=None)\n",
    "raw_org = pd.DataFrame(df.loc[df['tag'] == 'ORGANIZATION', 'tokenized_text'], index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "orgs = raw_org['tokenized_text'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_list = []\n",
    "orgs_list = []\n",
    "for item in orgs:\n",
    "    if item not in \"()*\": # assuming vast majority of organizatins delimited by round brackets\n",
    "        temp_list.append(item)\n",
    "    else:\n",
    "        orgs_list.append(' '.join(temp_list))\n",
    "        temp_list = []\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokenized_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Jiaolong</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Yang</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Wei</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Liang</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>Xin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>Tong</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>Sandra</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>Zilles</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>Yu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>Cheng</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>Yi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>Cheng</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>Jianshu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>Li</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>228</th>\n",
       "      <td>Jianmin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>229</th>\n",
       "      <td>Wang</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>Chenfei</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>Wu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>269</th>\n",
       "      <td>Xiaojie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270</th>\n",
       "      <td>Wang</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>280</th>\n",
       "      <td>Ruifan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>281</th>\n",
       "      <td>Li</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>311</th>\n",
       "      <td>Walter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>312</th>\n",
       "      <td>Karlen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>334</th>\n",
       "      <td>Ariel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>335</th>\n",
       "      <td>D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>336</th>\n",
       "      <td>Procaccia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>371</th>\n",
       "      <td>Kewen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>372</th>\n",
       "      <td>Wang</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>380</th>\n",
       "      <td>Zhe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51945</th>\n",
       "      <td>Hong</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51946</th>\n",
       "      <td>Chen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51947</th>\n",
       "      <td>(</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51956</th>\n",
       "      <td>Jimeng</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51957</th>\n",
       "      <td>Sun</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51974</th>\n",
       "      <td>Chang</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51975</th>\n",
       "      <td>Xu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51982</th>\n",
       "      <td>Weiran</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51983</th>\n",
       "      <td>Huang</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51984</th>\n",
       "      <td>(</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51985</th>\n",
       "      <td>Huawei</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51986</th>\n",
       "      <td>Noah</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51992</th>\n",
       "      <td>Hongwei</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51993</th>\n",
       "      <td>Wang</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52008</th>\n",
       "      <td>Tieyan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52009</th>\n",
       "      <td>Liu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52021</th>\n",
       "      <td>Lisi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52022</th>\n",
       "      <td>Chen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52028</th>\n",
       "      <td>Shuo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52029</th>\n",
       "      <td>Shang</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52104</th>\n",
       "      <td>Guoqi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52105</th>\n",
       "      <td>Li</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52111</th>\n",
       "      <td>Jun</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52112</th>\n",
       "      <td>Zhu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52129</th>\n",
       "      <td>Luping</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52130</th>\n",
       "      <td>Shi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52154</th>\n",
       "      <td>Katharina</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52155</th>\n",
       "      <td>Muelling</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52159</th>\n",
       "      <td>Katerina</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52160</th>\n",
       "      <td>Fragkiadaki</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4818 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      tokenized_text\n",
       "21          Jiaolong\n",
       "22              Yang\n",
       "32               Wei\n",
       "33             Liang\n",
       "41               Xin\n",
       "42              Tong\n",
       "64            Sandra\n",
       "65            Zilles\n",
       "134               Yu\n",
       "135            Cheng\n",
       "143               Yi\n",
       "144            Cheng\n",
       "168          Jianshu\n",
       "169               Li\n",
       "228          Jianmin\n",
       "229             Wang\n",
       "246          Chenfei\n",
       "247               Wu\n",
       "269          Xiaojie\n",
       "270             Wang\n",
       "280           Ruifan\n",
       "281               Li\n",
       "311           Walter\n",
       "312           Karlen\n",
       "334            Ariel\n",
       "335                D\n",
       "336        Procaccia\n",
       "371            Kewen\n",
       "372             Wang\n",
       "380              Zhe\n",
       "...              ...\n",
       "51945           Hong\n",
       "51946           Chen\n",
       "51947              (\n",
       "51956         Jimeng\n",
       "51957            Sun\n",
       "51974          Chang\n",
       "51975             Xu\n",
       "51982         Weiran\n",
       "51983          Huang\n",
       "51984              (\n",
       "51985         Huawei\n",
       "51986           Noah\n",
       "51992        Hongwei\n",
       "51993           Wang\n",
       "52008         Tieyan\n",
       "52009            Liu\n",
       "52021           Lisi\n",
       "52022           Chen\n",
       "52028           Shuo\n",
       "52029          Shang\n",
       "52104          Guoqi\n",
       "52105             Li\n",
       "52111            Jun\n",
       "52112            Zhu\n",
       "52129         Luping\n",
       "52130            Shi\n",
       "52154      Katharina\n",
       "52155       Muelling\n",
       "52159       Katerina\n",
       "52160    Fragkiadaki\n",
       "\n",
       "[4818 rows x 1 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_persons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
